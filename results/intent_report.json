{
  "private_consultation": {
    "precision": 0.9830508474576272,
    "recall": 0.9830508474576272,
    "f1-score": 0.9830508474576272,
    "support": 59,
    "confused_with": {
      "further_education": 1
    }
  },
  "graduation_time": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 7,
    "confused_with": {}
  },
  "error": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1,
    "confused_with": {}
  },
  "review_transcripts": {
    "precision": 0.9916492693110647,
    "recall": 1.0,
    "f1-score": 0.9958071278825995,
    "support": 475,
    "confused_with": {}
  },
  "Updating": {
    "precision": 0.981042654028436,
    "recall": 0.9764150943396226,
    "f1-score": 0.9787234042553191,
    "support": 212,
    "confused_with": {
      "review_transcripts": 2,
      "admissions_info": 1
    }
  },
  "cirriculum_introduction": {
    "precision": 0.9764705882352941,
    "recall": 0.9764705882352941,
    "f1-score": 0.9764705882352941,
    "support": 85,
    "confused_with": {
      "further_education": 1,
      "Updating": 1
    }
  },
  "education_fee": {
    "precision": 1.0,
    "recall": 0.8571428571428571,
    "f1-score": 0.923076923076923,
    "support": 21,
    "confused_with": {
      "further_education": 2,
      "admission_score": 1
    }
  },
  "unkown_quests": {
    "precision": 0.95,
    "recall": 1.0,
    "f1-score": 0.9743589743589743,
    "support": 19,
    "confused_with": {}
  },
  "mood_great": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 14,
    "confused_with": {}
  },
  "skills_test_info": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 154,
    "confused_with": {}
  },
  "further_education": {
    "precision": 0.956140350877193,
    "recall": 1.0,
    "f1-score": 0.9775784753363228,
    "support": 109,
    "confused_with": {}
  },
  "master_training": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 19,
    "confused_with": {}
  },
  "affirm": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 6,
    "confused_with": {}
  },
  "calculate": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 3,
    "confused_with": {}
  },
  "goodbye": {
    "precision": 0.9166666666666666,
    "recall": 1.0,
    "f1-score": 0.9565217391304348,
    "support": 11,
    "confused_with": {}
  },
  "greet": {
    "precision": 1.0,
    "recall": 0.9333333333333333,
    "f1-score": 0.9655172413793104,
    "support": 15,
    "confused_with": {
      "goodbye": 1
    }
  },
  "admission_score": {
    "precision": 0.9655172413793104,
    "recall": 1.0,
    "f1-score": 0.9824561403508771,
    "support": 56,
    "confused_with": {}
  },
  "bot_challenge": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 4,
    "confused_with": {}
  },
  "admissions_info": {
    "precision": 0.9981167608286252,
    "recall": 0.9833024118738405,
    "f1-score": 0.9906542056074767,
    "support": 539,
    "confused_with": {
      "Updating": 3,
      "review_transcripts": 2
    }
  },
  "graduate_job_oppotunity": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 9,
    "confused_with": {}
  },
  "mood_unhappy": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 14,
    "confused_with": {}
  },
  "deny": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 7,
    "confused_with": {}
  },
  "about_university": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 36,
    "confused_with": {}
  },
  "accuracy": 0.9888,
  "macro avg": {
    "precision": 0.9877675816862704,
    "recall": 0.9873789187992423,
    "f1-score": 0.9871398116117895,
    "support": 1875
  },
  "weighted avg": {
    "precision": 0.9890245209384544,
    "recall": 0.9888,
    "f1-score": 0.9887658109806192,
    "support": 1875
  }
}